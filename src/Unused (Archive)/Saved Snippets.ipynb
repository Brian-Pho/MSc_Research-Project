{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saved Snippets\n",
    "This script holds random snippets of code that I may need again, but are cluttering the main scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Yeo Functional Connectivity Using Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Based off of:\n",
    "- https://seaborn.pydata.org/examples/heat_scatter.html\n",
    "- https://seaborn.pydata.org/examples/many_pairwise_correlations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Search for all functional connectivity files and read them\n",
    "fc_path = '/imaging3/owenlab/bpho/python_yeo_fc'\n",
    "fc_paths = glob.glob(fc_path + '/**/yeo_fc.npy', recursive=True)\n",
    "\n",
    "fcs = {}\n",
    "for path in fc_paths:\n",
    "    subject_id = get_subject_from_path(path)\n",
    "    fcs[subject_id] = np.load(path)\n",
    "print(\"Number of functional connectivity:\", len(fcs))\n",
    "print(\"Number of features (connections):\", fcs[\"NDARAP912JK3\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(fcs[\"NDARAP912JK3\"], cmap=cmap, center=0,\n",
    "            square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Load the brain networks dataset, select subset, and collapse the multi-index\n",
    "df = sns.load_dataset(\"brain_networks\", header=[0, 1, 2], index_col=0)\n",
    "\n",
    "used_networks = [1, 5, 6, 7, 8, 12, 13, 17]\n",
    "used_columns = (df.columns\n",
    "                  .get_level_values(\"network\")\n",
    "                  .astype(int)\n",
    "                  .isin(used_networks))\n",
    "df = df.loc[:, used_columns]\n",
    "\n",
    "df.columns = df.columns.map(\"-\".join)\n",
    "\n",
    "# Compute a correlation matrix and convert to long-form\n",
    "corr_mat = df.corr().stack().reset_index(name=\"correlation\")\n",
    "\n",
    "# Draw each cell as a scatter point with varying size and color\n",
    "g = sns.relplot(\n",
    "    data=fcs[\"NDARAP912JK3\"],\n",
    "    x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n",
    "    palette=\"vlag\", hue_norm=(-1, 1), edgecolor=\".7\",\n",
    "    height=10, sizes=(50, 250), size_norm=(-.2, .8),\n",
    ")\n",
    "\n",
    "# Tweak the figure to finalize\n",
    "g.set(xlabel=\"\", ylabel=\"\", aspect=\"equal\")\n",
    "g.despine(left=True, bottom=True)\n",
    "g.ax.margins(.02)\n",
    "for label in g.ax.get_xticklabels():\n",
    "    label.set_rotation(90)\n",
    "for artist in g.legend.legendHandles:\n",
    "    artist.set_edgecolor(\".7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom PLSRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import pinv2\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _center_scale_xy(X, Y, scale=True):\n",
    "    \"\"\" Center X, Y and scale if the scale parameter==True\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        X, Y, x_mean, y_mean, x_std, y_std\n",
    "    \"\"\"\n",
    "    # center\n",
    "    x_mean = X.mean(axis=0)\n",
    "    X -= x_mean\n",
    "    y_mean = Y.mean(axis=0)\n",
    "    Y -= y_mean\n",
    "    # scale\n",
    "    if scale:\n",
    "        x_std = X.std(axis=0, ddof=1)\n",
    "        x_std[x_std == 0.0] = 1.0\n",
    "        X /= x_std\n",
    "        y_std = Y.std(axis=0, ddof=1)\n",
    "        y_std[y_std == 0.0] = 1.0\n",
    "        Y /= y_std\n",
    "    else:\n",
    "        x_std = np.ones(X.shape[1])\n",
    "        y_std = np.ones(Y.shape[1])\n",
    "    return X, Y, x_mean, y_mean, x_std, y_std\n",
    "\n",
    "\n",
    "class PLSRW():\n",
    "    \n",
    "    def __init__(self, n_components=2, scale=True, reg=0.01):\n",
    "        self.n_components=n_components\n",
    "        self.scale = scale\n",
    "        self.reg = reg\n",
    "    \n",
    "    def _calc_dist(self, X, Y):\n",
    "        dist = []\n",
    "        \n",
    "        for feature in range(X.shape[1]):\n",
    "            feature_dist = np.linalg.norm(Y - X[:, feature])\n",
    "            dist.append(feature_dist)\n",
    "        \n",
    "        return np.array(dist)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        Y = Y.astype('float64')\n",
    "        if Y.ndim == 1:\n",
    "            Y = Y.reshape(-1, 1)\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        p = X.shape[1]\n",
    "        q = Y.shape[1]\n",
    "        \n",
    "        n_components = self.n_components\n",
    "        reg = self.reg\n",
    "        eps = np.finfo(X.dtype).eps\n",
    "        Y_eps = np.finfo(Y.dtype).eps\n",
    "        \n",
    "        self.x_weights_ = np.zeros((p, n_components))  # U\n",
    "        self._x_scores = np.zeros((n, n_components))  # Xi\n",
    "        self.x_loadings_ = np.zeros((p, n_components))  # Gamma\n",
    "        self.y_loadings_ = np.zeros((q, n_components))  # Delta\n",
    "        \n",
    "        # Scale (in place)\n",
    "        Xk, Yk, self._x_mean, self._y_mean, self._x_std, self._y_std = (\n",
    "            _center_scale_xy(X, Y, self.scale))\n",
    "        Yk_mask = np.all(np.abs(Yk) < 10 * Y_eps, axis=0)\n",
    "        Yk[:, Yk_mask] = 0.0\n",
    "        \n",
    "        for k in range(n_components):\n",
    "            # Compute the regularization matrix\n",
    "            d = self._calc_dist(Xk, Yk)\n",
    "            D = np.diag(d)\n",
    "            print(reg * (D.T @ D))\n",
    "            print((Xk.T @ Xk))\n",
    "            \n",
    "            # Compute the PLSRW weight\n",
    "            w_inter = pinv2(\n",
    "                ((Xk.T @ Xk) + (reg * (D.T @ D))), check_finite=False)\n",
    "            x_weights = (w_inter @ Xk.T) @ Yk\n",
    "            print(\"x_weights:\", x_weights.shape)\n",
    "            \n",
    "            # Normalize weight\n",
    "            x_weights /= np.sqrt(x_weights.T @ x_weights) + eps\n",
    "            \n",
    "            # Calculate the corresponding scores and loadings\n",
    "            x_scores = Xk @ x_weights\n",
    "            x_loadings = (Xk.T @ x_scores) / (x_scores.T @ x_scores)\n",
    "            y_loadings = (Yk.T @ x_scores) / (x_scores.T @ x_scores)\n",
    "            \n",
    "            # Deflate X and Y\n",
    "            Xk -= np.outer(x_scores, x_loadings)\n",
    "            Yk -= np.outer(x_scores, y_loadings)\n",
    "            print(\"Xk:\", Xk.shape, \"Yk:\", Yk.shape)\n",
    "            \n",
    "            self.x_weights_[:, k] = x_weights[:, 0]\n",
    "            self._x_scores[:, k] = x_scores[:, 0]\n",
    "            self.x_loadings_[:, k] = x_loadings[:, 0]\n",
    "            self.y_loadings_[:, k] = y_loadings[:, 0]\n",
    "\n",
    "        # Compute transformation matrices\n",
    "        self.x_rotations_ = np.dot(\n",
    "            self.x_weights_,\n",
    "            pinv2(np.dot(self.x_loadings_.T, self.x_weights_),\n",
    "                  check_finite=False))\n",
    "        \n",
    "        self.coef_ = np.dot(self.x_rotations_, self.y_loadings_.T)\n",
    "        self.coef_ = self.coef_ * self._y_std\n",
    "        print(\"coef:\", self.coef_.shape)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.coef_\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plsrw = PLSRW(n_components=2, reg=0)\n",
    "plsrw.fit(X, y)\n",
    "print(plsrw.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLS Group-and-Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def group_and_average(X, y, group_size=3):\n",
    "    X_temp, y_temp = [], []\n",
    "    \n",
    "    # Multiply by 2 to enable the splits to be random\n",
    "    num_sections = X.shape[0] / (group_size * 2)\n",
    "    X_indices = np.arange(X.shape[0])\n",
    "    subarray_indices = np.array(np.split(X_indices, num_sections, axis=0))\n",
    "    \n",
    "    # Shuffle each section so each time the split is run, the groups are different\n",
    "    rng = np.random.default_rng()\n",
    "    rng.shuffle(subarray_indices, axis=1)\n",
    "    \n",
    "    for subarray in subarray_indices:\n",
    "        # Half each section to return to the original group size\n",
    "        front_half, back_half = subarray[:group_size], subarray[group_size:]\n",
    "        X_temp.append(np.mean(X[front_half], axis=0))\n",
    "        X_temp.append(np.mean(X[back_half], axis=0))\n",
    "        \n",
    "        y_temp.append(np.mean(y[front_half]))\n",
    "        y_temp.append(np.mean(y[back_half]))\n",
    "    \n",
    "    X_temp, y_temp = shuffle(X_temp, y_temp)\n",
    "    return np.array(X_temp), np.array(y_temp)\n",
    "\n",
    "\n",
    "def group_and_average_by_y(X, y, group_size=3):\n",
    "    # Sort data by target\n",
    "    sort_indices = np.argsort(y)\n",
    "    X, y = X[sort_indices], y[sort_indices]\n",
    "    \n",
    "    return group_and_average(X, y, group_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "num_component = 2\n",
    "train_scores, test_scores = [], []\n",
    "pearsonr_scores, spearmanr_scores = [], []\n",
    "coefs = []\n",
    "\n",
    "for j in range(0, 10):\n",
    "    X_group, y_group = group_and_average_by_y(X, y)\n",
    "    mi = SelectKBest(mutual_info_regression, k=3000)\n",
    "    X_group = mi.fit_transform(X_group, y_group)\n",
    "    \n",
    "    for i in range(0, 10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_group, y_group, test_size=0.3, shuffle=True)\n",
    "\n",
    "        pls = PLSRegression(n_components=num_component)\n",
    "        pls.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = pls.predict(X_train)\n",
    "        y_test_pred = pls.predict(X_test)[:, 0]\n",
    "\n",
    "        train_scores.append(r2_score(y_train, y_train_pred))\n",
    "        test_scores.append(r2_score(y_test, y_test_pred))\n",
    "        pearsonr_scores.append(stats.pearsonr(y_test, y_test_pred)[0])\n",
    "        spearmanr_scores.append(stats.spearmanr(y_test, y_test_pred)[0])\n",
    "        coefs.append(pls.coef_)\n",
    "\n",
    "avg_train_score, avg_test_score = np.mean(train_scores), np.mean(test_scores)\n",
    "avg_pearsonr, avg_spearmanr = np.mean(pearsonr_scores), np.mean(spearmanr_scores)\n",
    "avg_coef = np.mean(coefs, axis=0)\n",
    "\n",
    "print(f'Measure: {selected_measure}')\n",
    "print(f\"Avg train score: {avg_train_score:.3f}\")\n",
    "print(f\"Avg test score: {avg_test_score:.3f}\")\n",
    "print(f\"Avg pearson: {avg_pearsonr:.3f}\")\n",
    "print(f\"Avg spearman: {avg_spearmanr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLS Iteration with Percentage Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "num_component = 2\n",
    "train_scores, test_scores = [], []\n",
    "pearsonr_scores, spearmanr_scores = [], []\n",
    "coefs = []\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, shuffle=True)\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_train_pred = pipe.predict(X_train)\n",
    "    y_test_pred = pipe.predict(X_test)[:, 0]\n",
    "    \n",
    "    train_scores.append(r2_score(y_train, y_train_pred))\n",
    "    test_scores.append(r2_score(y_test, y_test_pred))\n",
    "    pearsonr_scores.append(stats.pearsonr(y_test, y_test_pred)[0])\n",
    "    spearmanr_scores.append(stats.spearmanr(y_test, y_test_pred)[0])\n",
    "    coefs.append(pls.coef_)\n",
    "    \n",
    "    if (i == 0):\n",
    "        print(X_train.shape, X_test.shape)\n",
    "\n",
    "avg_train_score, avg_test_score = np.mean(train_scores), np.mean(test_scores)\n",
    "avg_pearsonr, avg_spearmanr = np.mean(pearsonr_scores), np.mean(spearmanr_scores)\n",
    "avg_coef = np.mean(coefs, axis=0)\n",
    "\n",
    "print(f\"Avg train score: {avg_train_score:.4f}\")\n",
    "print(f\"Avg test score: {avg_test_score:.4f}\")\n",
    "print(f\"Avg pearson: {avg_pearsonr:.4f}\")\n",
    "print(f\"Avg spearman: {avg_spearmanr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Scorer with r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, explained_variance_score\n",
    "\n",
    "def regression_scorer(reg, X, y):\n",
    "    y_pred = reg.predict(X)[:, 0]\n",
    "    \n",
    "    return {'r2': r2_score(y, y_pred), 'pearsonr': stats.pearsonr(y, y_pred)[0],\n",
    "            'explained_variance': explained_variance_score(y, y_pred)}\n",
    "\n",
    "scoring = ['train_r2', 'test_r2', 'train_pearsonr', 'test_pearsonr', 'test_explained_variance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLS with Expanded K-fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats=1, random_state=251183)\n",
    "train_scores, test_scores = [], []\n",
    "pearsonr_scores, spearmanr_scores = [], []\n",
    "coefs = []\n",
    "\n",
    "for train_index, test_index in rkf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    pipe = make_pipeline(StandardScaler(), PLSRegression(n_components=2))\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_train_pred = pipe.predict(X_train)\n",
    "    y_test_pred = pipe.predict(X_test)[:, 0]\n",
    "    \n",
    "    train_scores.append(r2_score(y_train, y_train_pred))\n",
    "    test_scores.append(r2_score(y_test, y_test_pred))\n",
    "    pearsonr_scores.append(stats.pearsonr(y_test, y_test_pred)[0])\n",
    "    spearmanr_scores.append(stats.spearmanr(y_test, y_test_pred)[0])\n",
    "    coefs.append(pipe['plsregression'].coef_)\n",
    "\n",
    "avg_train_score, avg_test_score = np.mean(train_scores), np.mean(test_scores)\n",
    "avg_pearsonr, avg_spearmanr = np.mean(pearsonr_scores), np.mean(spearmanr_scores)\n",
    "avg_coef = np.mean(coefs, axis=0)\n",
    "\n",
    "print(f'Target: {selected_target}')\n",
    "print(f\"Avg train score: {avg_train_score:.2f}\")\n",
    "print(f\"Avg test score: {avg_test_score:.2f}\")\n",
    "print(f\"Avg pearson: {avg_pearsonr:.2f}\")\n",
    "print(f\"Avg spearman: {avg_spearmanr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLS with Multivariate Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "keys = Y.keys()\n",
    "# keys = ['WISC_VSI', 'WISC_FRI', 'WISC_WMI', 'WISC_PSI', 'WISC_VCI']\n",
    "# keys = 'WISC_BD_Scaled', 'WISC_Similarities_Scaled', 'WISC_MR_Scaled', 'WISC_DS_Scaled', 'WISC_Coding_Scaled', 'WISC_Vocab_Scaled', 'WISC_FW_Scaled', 'WISC_VP_Scaled', 'WISC_PS_Scaled', 'WISC_SS_Scaled'\n",
    "y = np.array(list(map(Y.get, keys))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pearsonr_scores = []\n",
    "\n",
    "for train_index, test_index in rkf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_test_pred = pipe.predict(X_test)\n",
    "    \n",
    "    pearsonr_temp = []\n",
    "    for wisc_measure_num in range(0, y_test_pred.shape[1]):\n",
    "        pearsonr_temp.append(stats.pearsonr(\n",
    "            y_test[:, wisc_measure_num], y_test_pred[:, wisc_measure_num])[0])\n",
    "        \n",
    "    pearsonr_scores.append(pearsonr_temp)\n",
    "\n",
    "pearsonr_scores = np.array(pearsonr_scores).T\n",
    "avg_test_scores = np.mean(pearsonr_scores, axis=1)\n",
    "\n",
    "for target, avg_test_score in zip(keys, avg_test_scores):\n",
    "    print(f'Target: {target} | r: {avg_test_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using permutation_test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def regression_scorer(reg, X, y):\n",
    "    y_pred = reg.predict(X)\n",
    "    scores = []\n",
    "    for measure in range(0, 6):\n",
    "        scores.append(stats.pearsonr(y[:, measure], y_pred[:, measure])[0])\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from common.binning import bin_by_age\n",
    "from common.wisc import FSIQ, PRIMARY_INDICES\n",
    "\n",
    "keys = FSIQ + PRIMARY_INDICES\n",
    "y = np.array(list(map(Y.get, keys))).T\n",
    "\n",
    "bins = bin_by_age(X, y, ages, y)\n",
    "bin_1, bin_2, bin_3 = bins[0], bins[1], bins[2]\n",
    "X_all = [X, bin_1[0], bin_2[0], bin_3[0]]\n",
    "y_all = [y, bin_1[1], bin_2[1], bin_3[1]]\n",
    "age_bin_label = [\"All  \", \"Bin 1\", \"Bin 2\", \"Bin 3\"]\n",
    "\n",
    "for X_cv, y_cv, bin_label in zip(X_all, y_all, age_bin_label):\n",
    "    score, _, pvalue = permutation_test_score(\n",
    "        pipe, X_cv, y_cv, cv=rkf, scoring=regression_scorer, n_permutations=3000, n_jobs=-1)\n",
    "    print(f'Bin: {bin_label} | Target: {target} | Score: {score:.2f} | p-value: {pvalue:.4f}')\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQ Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bins = bin_by_feature(X, y, y, 3)\n",
    "bin_1, bin_2, bin_3 = bins[0], bins[1], bins[2]\n",
    "print(f'Bin 1: {bin_1[0].shape} | Bin 2: {bin_2[0].shape} | Bin 3: {bin_3[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_noise_samples(X, y, num_times):\n",
    "    X_std = np.std(X, axis=0)\n",
    "    \n",
    "    for i in range(0, num_times):\n",
    "        X_noisy = X + np.random.normal(0, X_std, X.shape)\n",
    "        X, y = np.append(X, X_noisy, axis=0), np.append(y, y)\n",
    "    \n",
    "    return shuffle(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display PLS plots during or after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Store the train, test, predicted train, and predicted test targets\n",
    "y_train_store, y_train_pred_store = [], []\n",
    "y_test_store, y_test_pred_store = [], []\n",
    "\n",
    "train_scores.append(stats.pearsonr(y_train, y_train_pred)[0])\n",
    "test_scores.append(stats.pearsonr(y_test, y_test_pred)[0])\n",
    "y_train_store.append(y_train)\n",
    "y_train_pred_store.append(y_train_pred)\n",
    "y_test_store.append(y_test)\n",
    "y_test_pred_store.append(y_test_pred)\n",
    "\n",
    "# Select run most representative of final results\n",
    "selected_k = -2\n",
    "print(stats.pearsonr(y_train_store[selected_k], y_train_pred_store[selected_k])[0])\n",
    "print(stats.pearsonr(y_test_store[selected_k], y_test_pred_store[selected_k])[0])\n",
    "\n",
    "# Add LR to plot\n",
    "y_train = y_train_store[selected_k].reshape(-1, 1)\n",
    "y_test = y_test_store[selected_k].reshape(-1, 1)\n",
    "\n",
    "y_train_pred_lr = LinearRegression().fit(y_train, y_train_pred).predict(y_train)\n",
    "y_test_pred_lr = LinearRegression().fit(y_test, y_test_pred).predict(y_test)\n",
    "print(y_train.shape, y_train_pred_lr.shape, y_test.shape, y_test_pred_lr.shape)\n",
    "\n",
    "lin_reg_train_score = stats.pearsonr(y_train[:, 0], y_train_pred_lr)[0]\n",
    "lin_reg_test_score = stats.pearsonr(y_test[:, 0], y_test_pred_lr)[0]\n",
    "print(\"Train r:\", lin_reg_train_score)\n",
    "print(\"Test r:\", lin_reg_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "lin_reg_train = LinearRegression().fit(y_train, y_train_pred)\n",
    "y_train_pred_lin_reg = lin_reg_train.predict(y_train)\n",
    "\n",
    "lin_reg_test = LinearRegression().fit(y_test, y_test_pred)\n",
    "y_test_pred_lin_reg = lin_reg_test.predict(y_test)\n",
    "\n",
    "lin_reg_train_score = lin_reg_train.score(y_train, y_train_pred)\n",
    "lin_reg_test_score = lin_reg_test.score(y_test, y_test_pred)\n",
    "print(\"Train r^2:\", lin_reg_train_score)\n",
    "print(\"Test r^2:\", lin_reg_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_train, y_train_pred, alpha=0.3, color='black')\n",
    "plt.plot(y_train, y_train_pred_lin_reg, color='#897B61')\n",
    "plt.title(\"Training Set\")\n",
    "plt.xlabel(f'True {selected_target}')\n",
    "plt.ylabel(f'Predicted {selected_target}')\n",
    "# plt.annotate(f\"r-squared = {avg_train_score:.3f}\", (6, 16))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_test_pred, alpha=0.3, color='black')\n",
    "plt.plot(y_test, y_test_pred_lin_reg, color='#897B61')\n",
    "plt.title(\"Testing Set\")\n",
    "plt.xlabel(f'True {selected_target}')\n",
    "plt.ylabel(f'Predicted {selected_target}')\n",
    "# plt.annotate(f\"r-squared = {lin_reg_test_score:.2f}\", (60, 87))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.hist(x=test_scores, rwidth=0.95)\n",
    "plt.title(\"Test Score Distribution\")\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Number of Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Yeo FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# To display each class of connections (within and between)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(subject_fc)\n",
    "a = np.zeros((11, 11))\n",
    "a[fpn_indices] = subject_fc[fpn_indices]\n",
    "plt.imshow(a)\n",
    "b = np.zeros((8, 8))\n",
    "b[np.triu_indices(8, k=1)] = subject_fc[dmn_indices]\n",
    "plt.imshow(b)\n",
    "plt.imshow(subject_fc[:11, 11:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection (Poor Performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sel = VarianceThreshold(0.055)\n",
    "X = sel.fit_transform(X)\n",
    "print(\"X shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select k strongest connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def score_fc(X, y):\n",
    "    # Take the strongest correlations regardless of sign\n",
    "    sum_fc = np.absolute(np.sum(X, axis=0))\n",
    "    return sum_fc\n",
    "\n",
    "X = SelectKBest(score_fc, k=3400).fit_transform(X, y)\n",
    "print(\"X shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display MI Before and After Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "\n",
    "mi = SelectKBest(mutual_info_regression, k=1000)\n",
    "X_mi = mi.fit_transform(X, y)\n",
    "print(\"X_mi shape:\", X_mi.shape)\n",
    "\n",
    "mi_bin_1 = SelectKBest(mutual_info_regression, k=3000)\n",
    "X_bin_1 = mi_bin_1.fit_transform(bin_1[0], bin_1[1])\n",
    "\n",
    "mi_bin_2 = SelectKBest(mutual_info_regression, k=2000)\n",
    "X_bin_2 = mi_bin_2.fit_transform(bin_2[0], bin_2[1])\n",
    "\n",
    "mi_bin_3 = SelectKBest(mutual_info_regression, k=500)\n",
    "X_bin_3 = mi_bin_3.fit_transform(bin_3[0], bin_3[1])\n",
    "\n",
    "print(f'X_bin_1: {X_bin_1.shape} | X_bin_2: {X_bin_2.shape} | X_bin_3: {X_bin_3.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from common.plotting import create_power_fc_matrix, plot_fc_matrix, plot_fc_graph\n",
    "\n",
    "plot_fc_matrix(create_power_fc_matrix(X[1]), -1, 1)\n",
    "plot_fc_matrix(create_power_fc_matrix(mi.inverse_transform(X_1[1].reshape(1, -1))), -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download HBN Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from common.paths import BIOBANK_LABELS\n",
    "\n",
    "downloads = '/imaging3/owenlab/wilson/Healthy'\n",
    "\n",
    "temp = pd.read_csv(BIOBANK_LABELS + '/NoDiag_NoDL_Aug102021.csv').set_index('Identifiers')\n",
    "temp = temp.drop(columns='File_Downloaded')\n",
    "downloaded = [folder[4:] for folder in listdir(downloads) if folder.startswith(\"sub-\")]\n",
    "b = [1 for i in range(0, len(downloaded))]\n",
    "downloaded = pd.DataFrame({'Identifiers': downloaded, 'File_Downloaded': b}).set_index('Identifiers')\n",
    "# c = temp.index.tolist()\n",
    "# for d in downloaded:\n",
    "#     if d not in c:\n",
    "#         print(d)\n",
    "# display(downloaded)\n",
    "# display(temp)\n",
    "\n",
    "merged = pd.merge(temp, downloaded, left_index=True, right_index=True, how='left')\n",
    "merged = merged.fillna(0, axis=1)\n",
    "display(merged)\n",
    "# merged.to_csv(BIOBANK_LABELS + '/NoDiag_NoDL_Aug102021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def DownloadFile(url):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    r = requests.get(url)\n",
    "    print(r.status_code)\n",
    "    if r.status_code == 404:\n",
    "        return\n",
    "    \n",
    "    with open(local_filename, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Comorbid ADHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "diagnosis = subjects_with_labels[['assessment Diagnosis_ClinicianConsensus,DX_01',\n",
    "                       'assessment Diagnosis_ClinicianConsensus,DX_02',\n",
    "                       'assessment Diagnosis_ClinicianConsensus,DX_03',\n",
    "                       'assessment Diagnosis_ClinicianConsensus,DX_04',\n",
    "                       'assessment Diagnosis_ClinicianConsensus,DX_05',\n",
    "                       'assessment Diagnosis_ClinicianConsensus,DX_06',\n",
    "                       'assessment Diagnosis_ClinicianConsensus,DX_07',\n",
    "                       'assessment Diagnosis_ClinicianConsensus,DX_08',\n",
    "                       'assessment Diagnosis_ClinicianConsensus,DX_09',\n",
    "                       'assessment Diagnosis_ClinicianConsensus,DX_10']]\n",
    "\n",
    "diagnosis = diagnosis[diagnosis['assessment Diagnosis_ClinicianConsensus,DX_02'].isnull()]\n",
    "diagnosis = diagnosis[diagnosis['assessment Diagnosis_ClinicianConsensus,DX_01'].str.contains('ADHD', regex=False)]\n",
    "\n",
    "display(diagnosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model Weights Across Age Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from common.calculation import (\n",
    "    calc_cosine_similarity, calc_norm_euclidean, compare_age_similarity)\n",
    "from common.plotting import plot_age_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_ages_s, bin_1_s, bin_2_s, bin_3_s = np.clip(all_ages, np.min(all_ages), 0), np.clip(bin_1, np.min(bin_1), 0), np.clip(bin_2, np.min(bin_2), 0), np.clip(bin_3, np.min(bin_3), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "comparisons = compare_age_similarity(all_ages_s, bin_1_s, bin_2_s, bin_3_s, calc_cosine_similarity)\n",
    "np.fill_diagonal(comparisons, 1)\n",
    "print(comparisons)\n",
    "plot_age_comparisons(comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "distance_func = stats.spearmanr\n",
    "\n",
    "print(distance_func(all_ages, bin_1))\n",
    "print(distance_func(all_ages, bin_2))\n",
    "print(distance_func(all_ages, bin_3))\n",
    "print(distance_func(bin_1, bin_2))\n",
    "print(distance_func(bin_1, bin_3))\n",
    "print(distance_func(bin_2, bin_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in subject IDs with FC using age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from common.paths import POWER_FC\n",
    "\n",
    "data_subject_ids = set()\n",
    "\n",
    "for age in range(5, 19):\n",
    "    age_dir = f'{POWER_FC}/Age{age}'\n",
    "    \n",
    "    if not exists(age_dir):\n",
    "        continue\n",
    "    \n",
    "    curr_age_subjects = {folder[4:] for folder in listdir(age_dir) if folder.startswith(\"sub-\")}\n",
    "    data_subject_ids = data_subject_ids.union(curr_age_subjects)\n",
    "\n",
    "data = pd.DataFrame(data_subject_ids, columns=['subject_id']).set_index('subject_id')\n",
    "\n",
    "print(f'Number of subjects with FC: {len(data_subject_ids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check WISC subtests against age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Grab columns\n",
    "wisc_col = 'assessment WISC'\n",
    "wisc_raw_measures = [\n",
    "    'WISC_BD_Raw', \n",
    "    'WISC_Similarities_Raw', \n",
    "    'WISC_MR_Raw',\n",
    "    'WISC_DS_Raw',\n",
    "    'WISC_Coding_Raw',\n",
    "    'WISC_Vocab_Raw',\n",
    "    'WISC_FW_Raw',\n",
    "    'WISC_VP_Raw',\n",
    "    'WISC_PS_Raw',\n",
    "    'WISC_SS_Raw',\n",
    "]\n",
    "wisc_scaled_measure = [\n",
    "    'WISC_BD_Scaled', \n",
    "    'WISC_Similarities_Scaled', \n",
    "    'WISC_MR_Scaled',\n",
    "    'WISC_DS_Scaled',\n",
    "    'WISC_Coding_Scaled',\n",
    "    'WISC_Vocab_Scaled',\n",
    "    'WISC_FW_Scaled',\n",
    "    'WISC_VP_Scaled',\n",
    "    'WISC_PS_Scaled',\n",
    "    'WISC_SS_Scaled',\n",
    "]\n",
    "wisc_measures = np.array([wisc_raw_measures, wisc_scaled_measure]).flatten('F')\n",
    "\n",
    "clean_labels = clean_labels.dropna(subset=[f'assessment WISC,{measure}' for measure in wisc_measures])\n",
    "wisc_measures_data = {measure: clean_labels[f'{wisc_col},{measure}'].astype(int).to_numpy() \n",
    "                      for measure in wisc_measures}\n",
    "age = clean_labels['assessment Basic_Demos,Age'].astype(float)\n",
    "\n",
    "print(\"Num WISC measures:\", len(wisc_measures_data))\n",
    "print(\"Num subjects:\", len(age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for measure, data in wisc_measures_data.items():\n",
    "    lin_reg = stats.linregress(age, data)\n",
    "    print(f\"{measure} r-squared:: {lin_reg.rvalue**2:.4f}\")\n",
    "    print(f\"{measure} p-value: {lin_reg.pvalue}\")\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# X_cv = bin_1[0]\n",
    "# y_cv = bin_1[1]\n",
    "X_cv = X\n",
    "y_cv = y\n",
    "age_group = 'all'\n",
    "\n",
    "estimators = [StandardScaler(), RidgeCV(alphas=[a for a in range(5000, 55000, 5000)], \n",
    "                                        scoring=regression_scorer, cv=rkf)]\n",
    "pipe = make_pipeline(*estimators)\n",
    "pipe.fit(X_cv, y_cv)\n",
    "ridge_cv = pipe['ridgecv']\n",
    "\n",
    "print(f'Target: {selected_target} | Alpha: {ridge_cv.alpha_} | Score: {ridge_cv.best_score_:.2f}')\n",
    "\n",
    "estimators = [StandardScaler(), Ridge(alpha=ridge_cv.alpha_)]\n",
    "pipe = make_pipeline(*estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Permutation Test Score Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def bcustom_permutation_test_score(\n",
    "    estimator, G_0, G_1, G_2, cv_0, cv_1, cv_2, n_permutations=100, scorer=None):\n",
    "    \n",
    "    G_0_scores, G_1_scores, G_2_scores, G_0_scores_perm, G_1_scores_perm, G_2_scores_perm = b_custom_permutation_test_score(\n",
    "        clone(estimator), G_0[0], G_0[1], G_1[0], G_1[1], G_2[0], G_2[1], cv_0, cv_1, cv_2, n_permutations, scorer)\n",
    "            \n",
    "    G_0_score, G_1_score, G_2_score = np.mean(G_0_scores), np.mean(G_1_scores), np.mean(G_2_scores)\n",
    "    \n",
    "    G_0_scores_perm, G_1_scores_perm, G_2_scores_perm = np.array(G_0_scores_perm), np.array(G_1_scores_perm), np.array(G_2_scores_perm)\n",
    "        \n",
    "#     total_permutations = n_permutations * len(cv_0)\n",
    "    total_permutations = n_permutations\n",
    "    \n",
    "    G_0_pvalue = calc_pvalue(G_0_scores_perm, G_0_score, total_permutations)\n",
    "    G_1_pvalue = calc_pvalue(G_1_scores_perm, G_1_score, total_permutations)\n",
    "    G_2_pvalue = calc_pvalue(G_2_scores_perm, G_2_score, total_permutations)\n",
    "\n",
    "    return G_0_score, G_1_score, G_2_score, G_0_pvalue, G_1_pvalue, G_2_pvalue\n",
    "\n",
    "\n",
    "def b_custom_permutation_test_score(estimator, X_0, y_0, X_1, y_1, X_2, y_2, cv_0, cv_1, cv_2, n_permutations, scorer):\n",
    "    \n",
    "    G_0_scores, G_1_scores, G_2_scores = [], [], []\n",
    "    G_0_scores_perm, G_1_scores_perm, G_2_scores_perm = [], [], []\n",
    "    \n",
    "    for curr_cv in range(len(cv_0)):\n",
    "        # Train and test on the in-group data\n",
    "        cv_0_train, cv_0_test = cv_0[curr_cv][0], cv_0[curr_cv][1]\n",
    "        X_0_train, X_0_test = X_0[cv_0_train], X_0[cv_0_test]\n",
    "        y_0_train, y_0_test = y_0[cv_0_train], y_0[cv_0_test]\n",
    "        \n",
    "        estimator.fit(X_0_train, y_0_train)\n",
    "        G_0_scores.append(scorer(estimator, X_0_test, y_0_test))\n",
    "        \n",
    "        # Test on the out-group data\n",
    "        cv_1_test, cv_2_test = cv_1[curr_cv][1], cv_2[curr_cv][1]\n",
    "        X_1_test, y_1_test = X_1[cv_1_test], y_1[cv_1_test]\n",
    "        X_2_test, y_2_test = X_2[cv_2_test], y_2[cv_2_test]\n",
    "        \n",
    "        G_1_scores.append(scorer(estimator, X_1_test, y_1_test))\n",
    "        G_2_scores.append(scorer(estimator, X_2_test, y_2_test))\n",
    "        \n",
    "        # Test on the shuffled out-group data\n",
    "        rng = np.random.default_rng()\n",
    "        G_0_scores_perm_temp, G_1_scores_perm_temp, G_2_scores_perm_temp = [], [], []\n",
    "        for _ in range(n_permutations):\n",
    "            y_0_test_perm = rng.permutation(y_0_test)\n",
    "            y_1_test_perm = rng.permutation(y_1_test)\n",
    "            y_2_test_perm = rng.permutation(y_2_test)\n",
    "            \n",
    "            G_0_scores_perm_temp.append(scorer(estimator, X_0_test, y_0_test_perm))\n",
    "            G_1_scores_perm_temp.append(scorer(estimator, X_1_test, y_1_test_perm))\n",
    "            G_2_scores_perm_temp.append(scorer(estimator, X_2_test, y_2_test_perm))\n",
    "#             G_0_scores_perm.append(scorer(estimator, X_0_test, y_0_test_perm))\n",
    "#             G_1_scores_perm.append(scorer(estimator, X_1_test, y_1_test_perm))\n",
    "#             G_2_scores_perm.append(scorer(estimator, X_2_test, y_2_test_perm))\n",
    "            \n",
    "        G_0_scores_perm.append(np.mean(G_0_scores_perm_temp))\n",
    "        G_1_scores_perm.append(np.mean(G_1_scores_perm_temp))\n",
    "        G_2_scores_perm.append(np.mean(G_2_scores_perm_temp))\n",
    "        \n",
    "    return G_0_scores, G_1_scores, G_2_scores, G_0_scores_perm, G_1_scores_perm, G_2_scores_perm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "display_scores = []\n",
    "# for bin_score in bin_test_scores:\n",
    "#     display_scores.append([s['p_value'] for s in bin_score])\n",
    "for _ in bin_test_labels:\n",
    "    temp = []\n",
    "    temp.append([s[1] for s in bin_score])\n",
    "    temp = np.array(temp).flatten()\n",
    "    display_scores.append(temp)\n",
    "\n",
    "# print(len(bin_test_labels), len(display_scores))\n",
    "results = pd.DataFrame(dict(zip(bin_test_labels, display_scores)))\n",
    "\n",
    "ax = sns.boxplot(data=results, palette=\"Set2\")\n",
    "ax = sns.swarmplot(data=results, color=\"0.3\", order=bin_test_labels)\n",
    "ax.set_ylabel('Test Score (Pearson r)')\n",
    "ax.set_xlabel('Age Bins')\n",
    "ax.set_title('Cross Validation Results (Bin 1 -> Each Bin)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def custom_permutation_test_score(\n",
    "    estimator, G_0, G_1, G_2, cv_0, cv_1, cv_2, n_permutations=100, scorer=None):\n",
    "    \n",
    "    G_scores, G_scores_perm = _custom_permutation_test_score(\n",
    "        clone(estimator), G_0[0], G_0[1], G_1[0], G_1[1], G_2[0], G_2[1], cv_0, cv_1, cv_2, n_permutations, scorer)\n",
    "    \n",
    "    G_mean_scores, G_pvalues = [], []\n",
    "    total_permutations = n_permutations\n",
    "#     total_permutations = n_permutations * len(cv_0)\n",
    "\n",
    "    for scores, scores_perm in zip(G_scores, G_scores_perm):\n",
    "        mean_score = np.mean(scores)\n",
    "        G_mean_scores.append(mean_score)\n",
    "        \n",
    "        scores_perm = np.array(scores_perm)\n",
    "        pvalue = calc_pvalue(scores_perm, mean_score, total_permutations)\n",
    "        G_pvalues.append(pvalue)\n",
    "\n",
    "    return G_mean_scores, G_pvalues\n",
    "\n",
    "\n",
    "def _custom_permutation_test_score(estimator, X_0, y_0, X_1, y_1, X_2, y_2, cv_0, cv_1, cv_2, n_permutations, scorer):\n",
    "    # Index = group\n",
    "    G_scores = [[], [], []]\n",
    "    G_scores_perm = [[], [], []]\n",
    "    \n",
    "    for curr_cv in range(len(cv_0)):\n",
    "        # Train and test on the in-group data\n",
    "        cv_0_train, cv_0_test = cv_0[curr_cv][0], cv_0[curr_cv][1]\n",
    "        X_0_train, X_0_test = X_0[cv_0_train], X_0[cv_0_test]\n",
    "        y_0_train, y_0_test = y_0[cv_0_train], y_0[cv_0_test]\n",
    "        \n",
    "        estimator.fit(X_0_train, y_0_train)\n",
    "        G_scores[0].append(scorer(estimator, X_0_test, y_0_test))\n",
    "        \n",
    "        # Test on the out-group data\n",
    "        cv_1_test, cv_2_test = cv_1[curr_cv][1], cv_2[curr_cv][1]\n",
    "        X_1_test, y_1_test = X_1[cv_1_test], y_1[cv_1_test]\n",
    "        X_2_test, y_2_test = X_2[cv_2_test], y_2[cv_2_test]\n",
    "        \n",
    "        G_scores[1].append(scorer(estimator, X_1_test, y_1_test))\n",
    "        G_scores[2].append(scorer(estimator, X_2_test, y_2_test))\n",
    "        \n",
    "        # Test on the shuffled out-group data\n",
    "        rng = np.random.default_rng()\n",
    "        G_scores_perm_temp = [[], [], []]\n",
    "        X_tests = [X_0_test, X_1_test, X_2_test]\n",
    "        y_tests = [y_0_test, y_1_test, y_2_test]\n",
    "        \n",
    "        for _ in range(n_permutations):\n",
    "            for X_test, y_test, scores_temp in zip(X_tests, y_tests, G_scores_perm_temp):\n",
    "#             for X_test, y_test, scores_temp in zip(X_tests, y_tests, G_scores_perm):\n",
    "                y_test_perm = rng.permutation(y_test)\n",
    "                scores_temp.append(scorer(estimator, X_test, y_test_perm))\n",
    "        \n",
    "        for score_perm, score_temp in zip(G_scores_perm, G_scores_perm_temp):\n",
    "            score_perm.append(np.mean(score_temp))\n",
    "        \n",
    "    return G_scores, G_scores_perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICC Out-Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import pingouin as pg\n",
    "\n",
    "group_one = pd.DataFrame(np.squeeze(np.array(bin_2_coefs), axis=2))\n",
    "group_one['group'] = 'g1'\n",
    "\n",
    "group_two = pd.DataFrame(np.squeeze(np.array(bin_3_coefs), axis=2))\n",
    "group_two['group'] = 'g2'\n",
    "\n",
    "group_one_two = pd.concat([group_one, group_two])\n",
    "icc_data = pd.melt(group_one_two, id_vars='group', var_name='connection', \n",
    "                   value_name='weight', ignore_index=False)\n",
    "# display(icc_data)\n",
    "\n",
    "icc = pg.intraclass_corr(data=icc_data, targets='connection', raters='group', ratings='weight').round(3)\n",
    "icc.set_index(\"Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Prediction Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(f'n={N_PERM},', selected_target)\n",
    "print('r-values: {:.4f}, {:.4f}, {:.4f}'.format(*result[0]))\n",
    "print('p-values: {:.4f}, {:.4f}, {:.4f}'.format(*result[2]))\n",
    "print('mean permutation r-values: {:.4f}, {:.4f}, {:.4f}'.format(*[np.mean(perms) for perms in result[1]]))\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equal bin by feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def equal_bin_by_feature(X, y, feature, other_feature, num_bins=3):\n",
    "    # Create the bins\n",
    "    indices = np.arange(feature.shape[0])\n",
    "    bin_indices = np.split(indices, num_bins)\n",
    "    \n",
    "    # Sort the data based on the feature\n",
    "    sort_indices = np.argsort(feature)\n",
    "    X, y = _select_data(X, y, sort_indices)\n",
    "    \n",
    "    # Apply the bins to the sorted data\n",
    "    bins = [(X[bin_index], y[bin_index]) for bin_index in bin_indices]\n",
    "    \n",
    "    # Print bin statistics (age, sex, y)\n",
    "    all_stats = [other_feature]\n",
    "    for stat in all_stats:\n",
    "        sorted_stat = stat[sort_indices]\n",
    "        binned_stat = [sorted_stat[bin_index] for bin_index in bin_indices]\n",
    "        \n",
    "        for bin_num, feature_bin in enumerate(binned_stat):\n",
    "            print(f'Bin {bin_num} Range: {np.min(feature_bin):.2f} -> {np.max(feature_bin):.2f}')\n",
    "#             print(f'Bin {bin_num}: {np.unique(feature_bin, return_counts=True)}')\n",
    "        print('---')\n",
    "        \n",
    "    return bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Distribution of Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.displot(scores['test_r'], kde=True)\n",
    "g.axes[0][0].axvline(np.mean(scores['test_r']), ls=\"--\", label='mean', c='black')\n",
    "g.axes[0][0].axvline(np.median(scores['test_r']), ls=\"--\", label='median', c='red')\n",
    "g.axes[0][0].set_title(f'r distribution ridge_{population}_{selected_target}_{age_group}')\n",
    "g.axes[0][0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation with Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "def cross_validate_bootstrap(pipe, X, y, cv, scoring, n_samples):\n",
    "    scores = []\n",
    "    \n",
    "    for train_index, test_index in cv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        X_train, y_train = resample(X_train, y_train, n_samples=n_samples)\n",
    "#         print(X_train.shape, y_train.shape)\n",
    "        \n",
    "        pipe.fit(X_train, y_train)\n",
    "        scores.append(scoring(pipe, X_test, y_test))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge with MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# X_cv = bin_1[0]\n",
    "# y_cv = bin_1[1]\n",
    "X_cv = X\n",
    "y_cv = y\n",
    "bin_label = 'all'\n",
    "\n",
    "estimators = [StandardScaler(), MultiOutputRegressor(\n",
    "    RidgeCV(alphas=[a for a in range(5000, 55000, 5000)], scoring=regression_scorer, cv=rkf))]\n",
    "pipe = make_pipeline(*estimators)\n",
    "pipe.fit(X_cv, y_cv)\n",
    "ridge_cvs = pipe['multioutputregressor'].estimators_\n",
    "\n",
    "for target, ridge_cv in zip(WISC_LEVEL[5], ridge_cvs):\n",
    "    print(f'Target: {target} | Alpha: {ridge_cv.alpha_} | Score: {ridge_cv.best_score_:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for metric in SCORING:\n",
    "    metric_values = scores[metric]\n",
    "    print(f'Avg {metric}: {np.mean(metric_values):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Ridge for Predicting Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "\n",
    "X_cv = X\n",
    "y_cv = sex\n",
    "selected_target = \"sex\"\n",
    "age_group = 'all'\n",
    "\n",
    "estimators = [StandardScaler(), RidgeClassifierCV()]\n",
    "pipe = make_pipeline(*estimators)\n",
    "\n",
    "scores = cross_validate(pipe, X_cv, y_cv, cv=RKF_10_10, n_jobs=-1, \n",
    "                        return_train_score=False, return_estimator=False)\n",
    "\n",
    "print(f'ridge_{population}_{selected_target}_{age_group}')\n",
    "print(np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge with Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from os.path import join\n",
    "from common.binning import bin_data\n",
    "from common.wisc import WISC_LEVEL\n",
    "from common.paths import RIDGE_WEIGHTS, RIDGE_RESULTS\n",
    "from common.scoring import bootstrap_permutation_test_score\n",
    "\n",
    "results = []\n",
    "targets = WISC_LEVEL[5]\n",
    "target_alphas = [5000, 5000, 10000, 5000, 20000, 5000]\n",
    "\n",
    "for target, target_alpha in zip(targets, target_alphas):\n",
    "    y = Y[target]\n",
    "    X_all, y_all, bin_labels = bin_data(X, y)\n",
    "    \n",
    "    for X_cv, y_cv, bin_label in zip(X_all, y_all, bin_labels):      \n",
    "        estimators = [StandardScaler(), Ridge(alpha=target_alpha)]\n",
    "        pipe = make_pipeline(*estimators)\n",
    "        \n",
    "        score, permutation_scores, pvalue = bootstrap_permutation_test_score(\n",
    "            pipe, X_cv, y_cv, cv=RKF_10_10, scoring=unimetric_scorer, n_permutations=N_PERM, \n",
    "            n_jobs=-1, bootstrap_n=380)\n",
    "        \n",
    "        results.append({    \n",
    "            'Model': 'ridge',\n",
    "            'Population': population,\n",
    "            'Target': target,\n",
    "            'Bin': bin_label,\n",
    "            'Alpha': target_alpha,\n",
    "            'Score': score,\n",
    "            'P-value': pvalue,\n",
    "        })\n",
    "        print(results[-1])\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.round(4))\n",
    "# filename = f'ridge_cv_{population}.csv'\n",
    "# results_df.to_csv(join(RIDGE_RESULTS, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Permutation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "perms = np.array(perm_scores).reshape(9, N_PERM)\n",
    "perms = pd.DataFrame(perms)\n",
    "\n",
    "selected_results = results_df['Train'] == results_df['Test']\n",
    "\n",
    "true_score = results_df[selected_results]\n",
    "perm_score = perms[selected_results]\n",
    "perm_score['Bin'] = [1, 2, 3]\n",
    "perm_score = perm_score.melt(id_vars='Bin', var_name='Perm_num', value_name='r')\n",
    "\n",
    "display(true_score)\n",
    "display(perm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Discrete variant\n",
    "g = sns.displot(perm_score, x=\"r\", hue=\"Bin\", palette=\"Set2\", element=\"step\", col='Bin', legend=False)\n",
    "g.fig.subplots_adjust(top=0.85)\n",
    "g.fig.suptitle(f'Model: ridge, Target: {selected_target}, Train: Self, Test: Self, Population: ADHD, Num Perm: {N_PERM}')\n",
    "for col_val, ax in g.axes_dict.items():\n",
    "    bin_idx = col_val - 1\n",
    "    ax.axvline(true_score['Score'].iloc[bin_idx], ls=\"--\", color=sns.color_palette(\"Set2\")[bin_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Continuous variant\n",
    "g = sns.displot(perm_score, x=\"r\", hue=\"Bin\", palette=\"Set2\", kind=\"kde\", fill=True, col='Bin')\n",
    "for col_val, ax in g.axes_dict.items():\n",
    "    bin_idx = col_val - 1\n",
    "    ax.axvline(true_score['Score'].iloc[bin_idx], ls=\"--\", color=sns.color_palette(\"Set2\")[bin_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swarmplot variant\n",
    "g = sns.catplot(x='Bin', y='r', data=perm_score, kind=\"swarm\", palette=\"Set2\", alpha=.75, s=4)\n",
    "sns.swarmplot(x='Test', y='Score', data=true_score, ax=g.ax, color='black', alpha=1, marker='X', s=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
